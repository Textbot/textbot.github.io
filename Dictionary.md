---
title: Dictionary
---

### Русскоязычный словарь по нейронным сетям  
  
[Word Embedding](#WordEmbedding) \ [Continuous Bag of Words [CBOW]](#CBOW) \ [Skip-gram](#Skip-gram) \ [Bag of n-grams](#BagOfNGrams) \ [Recurrent Neural Network [RNN]](#RNN) \ [LSTM](#LSTM) \ [Bidirectional RNN](#BidirectionalRNN) \ [Seq2Seq](#Seq2Seq) \ [SPINN](#SPINN) \ [NetworkState](#NetworkState) \ [Batch](#Batch) \ [BLEU](#BLEU) \ [Arbitrarily distant inputs](#ArbitrarilyDistantInputs) \ [Truncated Backpropagation](#TruncatedBackpropagation) \ [One-hot vector](#One-hot vector) \ [Neighbourhood](#Neighbourhood) \ [Window](#Window)
  
<a name="WordEmbedding"></a>**Векторное представление слова** (от англ. Word Embedding) - подход к представлению формы и содержания слова, основанный на модели дистрибутивной семантики. Чаще всего под векторным представлением понимается модель типа Word2Vec. С математической точки зрения векторное представление слова суть отображение, ставящее в соответствие множеству слов некоторого языка множество точек в n-мерном векторном пространстве.

<a name="CBOW"></a>**Непрерывный мешок слов** (от англ. Continuous Bag of Words [CBOW]) - один из двух основных подходов (помимо Skip-gram) к построению нейросети, осуществляющей векторное представление слов. В основу подхода положена зависимость векторного представления слова от суммы векторных представлений слов из его [окна](#Window).

<a name="Skip-gram"></a>**Skip-грамма** (от англ. Skip-gram) - один из двух основных подходов (помимо CBOW) к построению нейросети, осуществляющей векторное представление слов. В основу подхода положена зависимость векторного представления слова от векторных представлений некоторых случайно выбранных слов из его [окна](#Window).

<a name="BagOfNGrams"></a>**Мешок n-грамм** (от англ. Bag of n-grams) Ссылка: <https://fasttext.cc/docs/en/supervised-tutorial.html#advanced-readers-what-is-a-bigram>

<a name="RNN"></a>**Рекуррентная нейронная сеть** [РНС] (от англ. Recurrent Neural Network [RNN]) - нейронная сеть, обрабатывающая входные данные последовательно и текущее состояние которой использует результаты предыдущей итерации

<a name="LSTM"></a>**LSTM-сеть** [РНС с долгой кратковременной памятью] (от англ. Long Short-term Memory Recurrent Neural Network [LSTM-RNN]) - рекуррентная нейронная сеть, обладающая одним или несколькими блоками долгой кратковременной памяти, позволяющими ей, в отличие от классических РНС, моделировать меру вхождения нового значения в память, меру сохранения значения в памяти и меру участия выходного значения в активации последующих элементов сети.

<a name="BidirectionalRNN"></a>**Двунаправленная РНС** (от англ. Bidirectional RNN) - РНС, представляющая собой две отдельных рекуррентных подсети, первая из которых проходит входных значения "слева-направа", а вторая - "справа-налево", выходные значения которых "объединяются".

<a name="Seq2Seq"></a>**Seq2Seq** (от англ. Sequence to Sequence) - РНС, преобразующая исходную последовательность в результирующую.

<a name="SPINN"></a>**SPINN-сеть** (от англ. Stack-Augmented Parser-Interpreter Neural Network)

<a name="NetworkState"></a>**Состояние сети** (от англ. Network State)

<a name="Batch"></a>**Партия обучающих данных** (от англ. Batch)

<a name="BLEU"></a>**BLEU** - метод автоматической оценки качества машинного перевода
        Ссылка: <http://www.aclweb.org/anthology/P02-1040.pdf>

<a name="ArbitrarilyDistantInputs"></a>**Сколь угодно большой вход (размер выборки)** (от англ. Arbitrarily distant inputs)

<a name="TruncatedBackpropagation"></a>**Усеченное обратное прохождение** (от англ. Truncated Backpropagation) -  метод обратного распространения ошибки в рекуррентных нейросетях, основанный на усечении последовательности элементов, участвующих с распространении ошибки. Ссылка: <https://www.youtube.com/watch?v=1xO37Tph57Y>

<a name="One-hot vector"></a>**Унитарный вектор** (от англ. One-hot vector) - булев вектор, содержащий только одну единицу. Такой вектор иногда называют прямым унитарным вектором. В свою очередь, вектор, содержащий только один нуль, называют обратным унитарным вектором.

<a name="Neighbourhood"></a>**Окрестность вектора** (от англ. Vacinity, Neighbourhood) - точки в n-мерном векторном пространстве, достаточно близкие к заданной. Используются, например, в векторных представления слов, когда необходимо найти слова, наиболее близкие к некоторому вектору. Если вектора некоторых слов достаточно близки к текущему вектору, мы можем называть их окрестностью данного вектора. Окрестность вектора может задаваться некоторым пороговым значением, однако, чаще всего, под окрестность понимается список из m наиболее близких векторов. 

<a name="Window"></a>**Окно вектора** (от англ. Window) - список из n слов, находящихся в предложении слева от текущего слова, и n слов, находящихся справа от него. Само значение n суть гиперпараметр.

**Функция активации слоя нейронов** (от англ. Activation function) - отображение, ставящее в соответствие множеству входных импульсов выходной импульс для каждого нейрона слоя.

**Метод обратного распространения ошибки** (от англ. BackPropagation) - метод (класс методов) обучения нейросетей, основанный на корректировке весов от выхода к входу.

**Коэффициент обучения** (от англ. Learning rate) - гиперпараметр нейросети, символизирующий скорость обучения сети (т.е. меру однократного изменения весов сети или слоя)

**Затухание коэффициента обучения** (от англ. Learning rate decay) {#lr_decay}

**Адаптивный коэффициент обучения** (от англ. Adaptive learning rate)

**Дропаут** (от англ. Dropout) - метод борьбы с переобучением нейросетей, основанный на случайном исключении некоторого множества нейронов из процесса обучения на конкретной итерации.

**Вероятность сохранения активности нейрона** (от англ. Dropout Keep probability) - коэффициент (гиперпараметр), указывающий, с какой вероятностью нейрон останется активен (не будет занулен) при дропауте. {#keep_prob}

**Целевое значение нейрона** (от англ. Target / Label) - "правильное" значение выходного нейрона

**Вычисление градиента** (от англ. Gradient computation) - 

**Взрыв градиента** (от англ. Overfitting)

**Затухание градиента** (от англ. Underfitting)

**Выравнивание градиента** (от англ. Gradient clipping) - метод борьбы с "взрывом" градиента.

**Метод отрицательной выборки** (от англ. Negative sampling) - метод обучения РНС типа Word2Vec, основанный на корректировке весов небольшой отрицательной выборки вместо всех отрицательных классов и позволяющий существенно ускорить процесс обучения нейросети. Ссылка: <http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/>

**Глубокое обучение** (от англ. Deep Learning) - обучение глубоких нейросетей, т.е. нейросетей с достаточно большим числом слоев нейронов.

**Широкое обучение** (от англ. Wide Learning) - обучение нейросетей с большим числом нейронов в слое признаков.

**Широкое глубокое обучение** (от англ. Deep & Wide Learning) - 

**Логит-функция** (от англ. Logit function) - 

**Эпоха обучения** (от англ. Training epoch) - одна итерация обучения нейросети на каждом примере обучающей выборки. Например, если партия обучающих данных (batch) состоит из 20 примеров, а обучающая выборка включает 100000 примеров, то одна эпоха будет состоять из 5000 циклов обучения нейросети.

**Перекрестная энтропия** (от англ. Cross entropy) - 

**Многозначная классификация** (от англ. Multilabel classification) - классификация, при которых входные данные относят сразу к нескольким классам. Ссылка: <https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits>

**Механизм "обращения внимания"** (от англ. Attention mechanism) - подход к обучения нейросетей типа Seq2Seq, основанном на сборе информации о соответствии в предложении слов различных языков друг другу. Обычно представляет собой декартово произведение множества слов предложения на исходном языке на множество слов предложения на целевом языке. Например, "A red house" - "Красный дом": здесь меры отношений <"A" - "Красный"> и <"A" - "дом"> стремятся к нулю, <"house" - "Красный"> и <"red" - "дом"> также не велики, а меры отношений  <"Red" - "Красный"> и <"house" - "дом"> стремятся к 1.

**Генеративно-состязательная сеть** (от англ. Generative adversarial network [GAN])

... (от англ. Candidate Sampling) Ссылка: <https://www.tensorflow.org/extras/candidate_sampling.pdf>

**Остаточная нейросеть** (от англ. Residual neural network) - нейросеть с возможность "перепрыгивания" между слоями. Предложена в 2015 году. Ссылка: <https://arxiv.org/pdf/1512.03385.pdf>

**Векторное представление фраз** (от англ. Learning Phrases) - метод, ставящий в соответствие фразе (токену) точку в n-мерном (например, 300-мерном) пространстве векторных представлений слов и фраз. Предложено в 2013 году Т.Миколовым, И.Суцкевером и др. Ссылка: <https://arxiv.org/pdf/1310.4546.pdf>

**Метод k-ближайших соседей** (от англ. k-nearest neighbors algorithm [k-nn]) - метод классификации и регрессии, основанные на анализе точек пространства в некоторой окрестности.

**Метод обратных взвешанных расстояний (Метод Шапарда)** (от англ. Inverse distance weighting) - метод многомерной интерполяции, регрессии в многомерных данных, используемых для поиска средних значений в n-мерных пространствах.

**Аналогические рассуждения (или рассуждения на основе аналогий)** (от англ. Analogical reasoning) - подход к идентификации синтаксических и семантических отношений в тексте на основе аналогий, предложенный в 2013-м году Т.Миколовым и коллегами. Ссылка: <https://www.aclweb.org/anthology/N13-1090>

**Метод главных компонент** (от англ. Principal Component Analysis) - метод понижения размерности данных с минимальными потерями точности. Имеет множество приложений, в т.ч. для визуализации данных. 

**t-SNE** (от англ. T-distribution Stochastic Neighbor Embedding) - метод понижения размерности многомерного пространства для визуализации. Обычно, пространство понижается до 2-мерного или 3-мерного. С математической точки зрение метод t-SNE суть отображение, ставящее в соответствие каждой точке n-мерного пространства единственную точку в k-мерном пространстве, где k<n и разница сумм всех евклидовых расстояний исходной и результирующей моделей меньше некоторого eps.

**Синтаксическое дерево составляющих** (от англ. Constituency Parse Tree) - одна из двух основных (наряду с синтаксическим деревом зависимостей) моделей представления синтаксиса предложения.

**Синтаксическое дерево зависимостей** (от англ. Dependency Parse Tree) - одна из двух основных (наряду с синтаксическим деревом составляющих) моделей представления синтаксиса предложения.

**Сжатие модели векторных представлений** (от англ. Model Compression) - процесс понижения емкостных характеристик модели векторных представлений.

**Векторное квантование** (от англ. Vector Quantization) - процесс квантования точек многомерного векторного пространства. Используется в т.ч. для сжатия модели векторных представлений.

**Продукционное квантование** (от англ. Product Quantization / Product Quantizer) - метод сжатия многомерных данных с использованием центроидов.
Ссылка 1: <http://mccormickml.com/2017/10/13/product-quantizer-tutorial-part-1/>
Ссылка 2: <https://github.com/facebookresearch/fastText/blob/25d0bb04bf43d8b674fe9ae5722ef65a0856f5d6/website/blog/2017-10-02-blog-post.md>

**Метод k-средних** (от англ. k-Means Clustering) - метод кластеризации и сжатия многомерных данных, основанный на выделении некоторого наперед заданного числа (обычно кратно типу данных, например 256 для типа byte) точек, являющихся центроидами для своих кластеров.
Ссылка: <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html>

**Быстрый метод k-средних** (от англ. Fast k-Means Clustering или Mini-Batch k-Means Clustering) - быстрый метод кластеризации и сжатия многомерных данных, в основу которого положен поиск центроидов в небольших случайно выбранных партиях данных.
Ссылка 1: <http://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf>
Ссылка 2: <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html>

**Диаграмма Вороного** (от англ. Voronoi Cell) - разбиение k-мерного пространства с конечным множеством точек S на n (где n суть мощность множества S) k-мерных областей D, где каждый элемент d_i из D_i всегда находится ближе к s_i, чем к любому другому элементу множества S.

**Нулевое обучение** (от англ. Zero-shot learning) - обучение, при котором система обучается решению задачи классификации без обучающих примеров, содержащих указание классов. В частности, речь идет об обучении без учителя для задач, по формулировке предполагающих обучение с учителем.
Ссылка: <https://arxiv.org/pdf/1508.07909.pdf>

**Обучение с первого раза** (от англ. One-shot learning) - обучение на одном или нескольких примерах, в отличие от традиционных методов
обучения, требующих тысячи или миллионы обучающих примеров.

**Частотное кодирование слов** (от англ. Byte Pair Encoding [BPE]) - метод кодирования слов подсловами на основе частоты их употребления в языке. Наиболее часто употребимые слова кодируются одним индексом, мало употребимые слова делятся на подслова, каждый из которых имеет собственный индекс. Данный метод, с одной стороны, позволяет сократить размер словаря, с другой стороны позволяет более точно моделировать дистрибутивную семантику неизвестных слов. Активно используется в нейросетях архитектуры Transformer для формирования входных последовательностей в задачах NLP.

**Тонкая настройка (дообучение) нейросети** (от англ. Fine-tuning) - подход к построению и обучению нейросетей, основанный на использовании предобученной нейросети для решения практических задач. В процессе Fine-tuning'а к предобученной сети могут добавляться дополнительные слои или дообучаться некоторые отдельные элементы сети для достижения лучших показателей работы сети для конкретной задачи. Ярким пример такого подхода является архитектура Transformer, позволяющая решать множество задач в области NLP и генерации информации с использованием одной предобученной нейросети, например, BERT или GPT-2.

**Алгоритм Краскала** (от англ. Kruskal algorithm) - алгоритм поиска минимального остовного дерева. Активно применяется в NLP, 
в частности в задачах выявления и представления синтаксических и семантических структур в тексте. В теоретикографовой терминологии, на вход получает матрицу смежности графа, а на выход передает матрицу смежности минимального (максимального - при домножении матрицы смежности графа на (-1.0)) остовного дерева.

<a name="Chunking"></a>**Чанкинг** (от англ. Chunking / Shallow parsing ["Поверностный парсинг"]) - задача NLP, состоящая в выделении словосочетаний из текста, в частности существительных и глагольных фраз.

**Распознавание именованных сущностей** (от англ. Named Entity Recognition [NER]) - задача NLP, состоящая в выделении сегментов/чанков (см. [Чанкинг](#Chunking)) текста и присвоении каждому из них некоторого типа сущности, например, PER (имя собственное), LOC (месторасположение), ORG (организация), MISC (разное), O (несущностный) и т.д. Обычно первый токен чанка помечается символом B (Beginning), например, B-PER, а последующие символом I (Inside), например, I-PER, т.е., скажем, чанку 'Nikita Nikitin' будет соответствовать сущность <B-PER, I-PER>. 
Ссылка: <http://nlpprogress.com/english/named_entity_recognition.html>

**Нормализация текста** (от англ. Text Normalization) - задача NLP, связанная с проблемой синтеза правдоподобного текста. Обычно нормализация является последним этапом генерации текста. В процессе нормализации могут, в частности, расставляться знаки препинания, заглавные буквы, согласовываться слова по морфологическим признакам и т.д. Важно понимать, что нормализация текста применяется лишь в моделях, где в обучающих выборках отсутствовали нормализуемые признаки. Например, если на вход нейросети подается текст без знаков препинания, то, соответственно, обучиться их расстановке она не может и, в случае синтеза текста, понадобится его нормализация. 

**Принудительное обучение с учителем** (от англ. Teacher-forcing training) - обучение НС задачам на последовательностях, при котором на n+1-й шаг передается правильное, а не выходное значение n. Например, в задаче машинного перевода, при обучении мы имеем предложение на английском языке и его правильных перевод на русский. При обучении на n-ом шаге НС прогнозирует перевод n-ого токена. Мы сравниваем его с правильных ответом, обучаем сеть в соотвествии с полученной ошибкой. Но на n+1-й шаг передаем правильное значение. 
