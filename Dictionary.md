---
title: Dictionary
---

### Русскоязычный словарь по нейронным сетям  
  
[Word Embedding](#WordEmbedding) \ [Continuous Bag of Words [CBOW]](#CBOW) \ [Recurrent Neural Network [RNN]](#RNN) \ [Bidirectional RNN](#BidirectionalRNN) \ [Seq2Seq](#Seq2Seq) \ [Skip-gram](#Skip-gram) \ [Truncated Backpropagation](#TruncatedBackpropagation) \ 
  
**Векторное представление слова** (от англ. Word Embedding<a name="WordEmbedding"></a>) - подход к представлению формы и содержания слова, основанный на модели дистрибутивной семантики. Чаще всего под векторным представлением понимается модель типа Word2Vec. С математической точки зрения векторное представление слова суть отображение, ставящее в соответствие множеству слов некоторого языка множество точек в n-мерном векторном пространстве.

**Непрерывный мешок слов** (от англ. Continuous Bag of Words [CBOW]<a name="CBOW"></a>)

**Рекуррентная нейронная сеть** [РНС] (от англ. Recurrent Neural Network [RNN]<a name="RNN"></a>) - нейронная сеть, обрабатывающая входные данные последовательно и текущее состояние которой использует результаты предыдущей итерации

**Двунаправленная РНС** (от англ. Bidirectional RNN<a name="BidirectionalRNN"></a>) - РНС, представляющая собой две отдельных рекуррентных подсети, первая из которых проходит входных значения "слева-направа", а вторая - "справа-налево", выходные значения которых "объединяются".

**Seq2Seq** (от англ. Sequence to Sequence<a name="Seq2Seq"></a>) - РНС, преобразующая исходную последовательность в результирующую.

**Skip-грамма** (от англ. Skip-gram<a name="Skip-gram"></a>)

**SPINN-сеть** (от англ. Stack-Augmented Parser-Interpreter Neural Network)

**Состояние сети** (от англ. Network State)

**Партия обучающих данных** (от англ. Batch)

**BLEU** - метод автоматической оценки качества машинного перевода
        Ссылка: <http://www.aclweb.org/anthology/P02-1040.pdf>

**Сколь угодно большой вход (размер выборки)** (от англ. arbitrarily distant inputs)

**Усеченное обратное прохождение** (от англ. Truncated Backpropagation<a name="TruncatedBackpropagation"></a>) -  метод обратного распространения ошибки в рекуррентных нейросетях, основанный на усечении последовательности элементов, участвующих с распространении ошибки.
        Ссылка: <https://www.youtube.com/watch?v=1xO37Tph57Y>

**Унитарный вектор** (от англ. One-hot vector) - булев вектор, содержащий только одну единицу. Такой вектор иногда называют прямым унитарным вектором. В свою очередь, вектор, содержащий только один нуль, называют обратным унитарным вектором.

**Окрестность вектора** (от англ. Vacinity, Neighbourhood) - точки в n-мерном векторном пространстве, достаточно близкие к заданной. Используются, например, в векторных представления слов, когда необходимо найти слова, наиболее близкие к некоторому вектору. Если вектора некоторых слов достаточно близки к текущему вектору, мы можем называть их окрестностью данного вектора. Окрестность вектора может задаваться некоторым пороговым значением, однако, чаще всего, под окрестность понимается список из m наиболее близких векторов. 

**Окно вектора** (от англ. Window) ...

**Функция активации слоя нейронов** (от англ. Activation function) - отображение, ставящее в соответствие множеству входных импульсов выходной импульс для каждого нейрона слоя.

**Метод обратного распространения ошибки** (от англ. BackPropagation) - метод (класс методов) обучения нейросетей, основанный на корректировке весов от выхода к входу.

**Коэффициент обучения** (от англ. Learning rate) - гиперпараметр нейросети, символизирующий скорость обучения сети (т.е. меру однократного изменения весов сети или слоя)

**Затухание коэффициента обучения** (от англ. Learning rate decay) {#lr_decay}

**Адаптивный коэффициент обучения** (от англ. Adaptive learning rate)

**Дропаут** (от англ. Dropout) - метод борьбы с переобучением нейросетей, основанный на случайном исключении некоторого множества нейронов из процесса обучения на конкретной итерации.

**Вероятность сохранения активности нейрона** (от англ. Dropout Keep probability) - коэффициент (гиперпараметр), указывающий, с какой вероятностью нейрон останется активен (не будет занулен) при дропауте. {#keep_prob}

**Целевое значение нейрона** (от англ. Target / Label) - "правильное" значение выходного нейрона

**Вычисление градиента** (от англ. Gradient computation) - 

**Взрыв градиента** (от англ. Overfitting)

**Затухание градиента** (от англ. Underfitting)

**Выравнивание градиента** (от англ. Gradient clipping) - метод борьбы с "взрывом" градиента.

**Метод отрицательной выборки** (от англ. Negative sampling) - метод обучения РНС типа Word2Vec, основанный на корректировке весов небольшой отрицательной выборки вместо всех отрицательных классов и позволяющий существенно ускорить процесс обучения нейросети. Ссылка: <http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/>

**Глубокое обучение** (от англ. Deep Learning) - обучение глубоких нейросетей, т.е. нейросетей с достаточно большим числом слоев нейронов.

**Широкое обучение** (от англ. Wide Learning) - обучение нейросетей с большим числом нейронов в слое признаков.

**Широкое глубокое обучение** (от англ. Deep & Wide Learning) - 

**Логит-функция** (от англ. Logit function) - 

**Эпоха обучения** (от англ. Training epoch) - одна итерация обучения нейросети на каждом примере обучающей выборки. Например, если партия обучающих данных (batch) состоит из 20 примеров, а обучающая выборка включает 100000 примеров, то одна эпоха будет состоять из 5000 циклов обучения нейросети.

**Перекрестная энтропия** (от англ. Cross entropy) - 

**Многозначная классификация** (от англ. Multilabel classification) - классификация, при которых входные данные относят сразу к нескольким классам. Ссылка: <https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits>

**Механизм "обращения внимания"** (от англ. Attention mechanism) - подход к обучения нейросетей типа Seq2Seq, основанном на сборе информации о соответствии в предложении слов различных языков друг другу. Обучно представляет собой декартово произведение множества слов предложения на исходном языке на множество слов предложения на целевом языке. Например, "A red house" - "Красный дом": здесь меры отношений <"A" - "Красный"> и <"A" - "дом"> стремятся к нулю, <"house" - "Красный"> и <"red" - "дом"> также не велики, а меры отношений  <"Red" - "Красный"> и <"house" - "дом"> стремятся к 1.

**Генеративно-состязательная сеть** (от англ. Generative adversarial network [GAN])

... (от англ. Candidate Sampling) Ссылка: <https://www.tensorflow.org/extras/candidate_sampling.pdf>

**Остаточная нейросеть** (от англ. Residual neural network) - нейросеть с возможность "перепрыгивания" между слоями. Предложена в 2015 году. Ссылка: <https://arxiv.org/pdf/1512.03385.pdf>

**Векторное представление фраз** (от англ. Learning Phrases) - метод, ставящий в соответствие фразе (токену) точку в n-мерном (обычно, 300-мерном) пространстве векторных представлений слов и фраз. Предложено в 2013 году Т.Миколовым, И.Суцкевером и др. Ссылка: <https://arxiv.org/pdf/1310.4546.pdf>

**Метод k-ближайших соседей** (от англ. k-nearest neighbors algorithm [k-nn]) - метод классификации и регрессии, основанные на анализе точек пространства в некоторой окрестности.

**Метод обратных взвешанных расстояний (Метод Шапарда)** (от англ. Inverse distance weighting) - метод многомерной интерполяции, регрессии в многомерных данных, используемых для поиска средних значений в n-мерных пространствах.

**Аналогические рассуждения (или рассуждения на основе аналогий)** (от англ. Analogical reasoning) - подход к идентификации синтаксических и семантических отношений в тексте на основе аналогий, предложенный в 2013-м году Т.Миколовым и коллегами. Ссылка: <https://www.aclweb.org/anthology/N13-1090>

**t-SNE** (от англ. T-distribution Stochastic Neighbor Embedding) - метод понижения размерности многомерного пространства для визуализации. Обычно, пространство понижается до 2-мерного или 3-мерного. С математической точки зрение метод t-SNE суть отображение, ставящее в соответствие каждой точке n-мерного пространства единственную точку в k-мерном пространстве, где k<n и разница сумм всех евклидовых расстояний исходной и результирующей моделей меньше некоторого eps.
