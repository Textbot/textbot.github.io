---
title: Русскоязычный терминологический словарь нейронных сетей
---

*Заметка: поскольку наш блог посвящен в первую очередь задачам NLP, мы будем ориентироваться в основном на соответствующую нейросетевую терминологию.*



Векторное представление слова (от англ. Word Embedding)

Непрерывный мешок слов (от англ. Continuous Bag of Words)

Рекуррентная нейронная сеть [РНС] (от англ. Recurrent Neural Network) - нейронная сеть, обрабатывающая входные данные последовательно и текущее состояние которой использует результаты предыдущей итерации

   Двунаправленная РНС (от англ. Bidirectional RNN) - РНС, представляющая собой две отдельных рекуррентных подсети, первая из которых проходит входных значения "слева-направа", а вторая - "справа-налево", выходные значения которых "объединяются".

Seq2Seq (от англ. Sequence to Sequence) - РНС, преобразующая исходную последовательность в результирующую.

Skip-грамма (от англ. Skip-gram)

SPINN-сеть (от англ. Stack-Augmented Parser-Interpreter Neural Network)

Состояние сети (от англ. Network State)

Партия обучающих данных (от англ. Batch)

BLEU - метод автоматической оценки качества машинного перевода
        Ссылка: http://www.aclweb.org/anthology/P02-1040.pdf

Сколь угодно большой вход (размер выборки) (от англ. arbitrarily distant inputs)

Усеченное обратное прохождение (от англ. Truncated Backpropagation) -  метод обратного распространения ошибки в рекуррентных нейросетях, основанный на усечении последовательности элементов, участвующих с распространении ошибки.
        Ссылка: https://www.youtube.com/watch?v=1xO37Tph57Y

Унитарный вектор (от англ. One-hot vector) - булев вектор, содержащий только одну единицу. Такой вектор иногда называют прямым унитарным вектором. В свою очередь, вектор, содержащий только один нуль, называют обратным унитарным вектором.

Окрестность вектора (от англ. Vacinity, Neighbourhood) - точки в n-мерном векторном пространстве, достаточно близкие к заданной. Используются, например, в векторных представления слов, когда необходимо найти слова, наиболее близкие к некоторому вектору. Если вектора некоторых слов достаточно близки к текущему вектору, мы можем называть их окрестностью данного вектора. Окрестность вектора может задаваться некоторым пороговым значением, однако, чаще всего, под окрестность понимается список из m наиболее близких векторов. 

Окно вектора (от англ. Window) ...

Функция активации слоя нейронов (от англ. Activation function) - отображение, ставящее в соответствие множеству входных импульсов выходной импульс для каждого нейрона слоя.

Метод обратного распространения ошибки (от англ. BackPropagation) - метод (класс методов) обучения нейросетей, основанный на корректировке весов от выхода к входу.

Коэффициент обучения (от англ. Learning rate) = скорость обучения

Затухание коэффициента обучения (от англ. Learning rate decay) {#lr_decay}

Адаптивный коэффициент обучения (от англ. Adaptive learning rate)

Дропаут (от англ. Dropout) - метод борьбы с переобучением нейросетей, основанный на случайном исключении некоторого множества нейронов из процесса обучения на конкретной итерации.

      Вероятность сохранения активности нейрона (от англ. Dropout Keep probability) - коэффициент (гиперпараметр), указывающий, с какой вероятностью нейрон останется активен (не будет занулен) при дропауте. {#keep_prob}

Целевое значение нейрона (от англ. Target / Label) - "правильное" значение выходного нейрона

Вычисление градиента (от англ. Gradient computation) - 

Выравнивание градиента (от англ. Gradient clipping) - метод борьбы с "взрывом" градиента.

Метод отрицательной выборки (от англ. Negative sampling) - метод обучения РНС типа Word2Vec, основанный на корректировке весов небольшой отрицательной выборки вместо всех отрицательных классов и позволяющий существенно ускорить процесс обучения нейросети.

Глубокое обучение (от англ. Deep Learning) - обучение глубоких нейросетей, т.е. нейросетей с достаточно большим числом слоев нейронов.

Широкое обучение (от англ. Wide Learning) - обучение нейросетей с большим числом нейронов в слое признаков.

Широкое глубокое обучение (от англ. Deep & Wide Learning) - 

Логит-функция (от англ. Logit function) - 

Эпоха обучения (от англ. Training epoch) - одна итерация обучения нейросети на каждом примере обучающей выборки. Например, если партия обучающих данных (batch) состоит из 20 примеров, а обучающая выборка включает 100000 примеров, то одна эпоха будет состоять из 5000 циклов обучения нейросети.

Перекрестная энтропия (от англ. Cross entropy) - 
