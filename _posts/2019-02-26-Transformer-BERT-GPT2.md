---
title: Нейросети архитектуры Transformer
---
На сегодняшний день (февраль 2019) лучшие результаты в решении основных задач NLP получены нейросетями т.н. архитектуры Transformer, предложенной исследователями компании Google в декабре 2017-го году в статье [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf). За этим последовала публикация материалов по GPT от OpenAI, затем последовал ответ от Google с нейросетью BERT, после чего OpenAI выложили сильно урезанную версию GPT-2, заявив, что общество не готово к публикации всей предобученной нейросети GPT-2.

Давайте разберемся, что из себя представляет нейросеть типа Transformer и чем она может быть нам полезна.
Первое, что бросается в глаза: Transformer - НЕ рекуррентная сеть. Забавно, что 3 дня назад (23.02.2019) вышла статья частично тех же авторов [Universal Transformers](https://arxiv.org/pdf/1807.03819.pdf), где предложена рекуррентная сеть архитектуры Transformer.
