---
title: Нейросети архитектуры Transformer
---
На сегодняшний день (февраль 2019) лучшие результаты в решении основных задач NLP получены нейросетями т.н. архитектуры Transformer, предложенной исследователями компании Google в декабре 2017-го году в статье [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf). За этим последовала публикация материалов по GPT от OpenAI, затем последовал ответ от Google с нейросетью BERT, после чего OpenAI выложили сильно урезанную версию GPT-2, заявив, что общество не готово к публикации всей предобученной нейросети GPT-2.

Давайте разберемся, что из себя представляет нейросеть типа Transformer и чем она может быть нам полезна.
Первое, что бросается в глаза: Transformer - НЕ рекуррентная сеть. Забавно, что 3 дня назад (23.02.2019) вышла статья частично тех же авторов [Universal Transformers](https://arxiv.org/pdf/1807.03819.pdf), где предложена рекуррентная сеть архитектуры Transformer.

![Архитектура Transformer](/img/Transformer.png)

Нейросети на архитектуре Transformer могут использоваться для самых разных задач NLP, однако основными "учебными" задачами будут восстановление пропущенных слов в предложении и оценка логического следования одного предложения из другого. По некоторой причине, для кодирования предложения или их пар (далее будем говорить "текст") используются вектора длины 512. Представление текста состоит из трех векторов длины 512 и осуществляется следующим образом: 

 - вектор token_input начиная с 0-й позиции содержит индексы "токенов", остальные элементы заполняются нулями. Под токенами могут пониматься словоформы, словосочетания, а также части словоформ, например, приставки, корни или окончания. Так, например, если текст разбит на 10 "токенов", то в векторе token_input первые 10 позиций заполняются индексами "токенов" из словаря, а остальные - нулями;

 - вектор mask_input кодирует пропущенные слова (т.н. "маски" [MASK]): если тот или иной "токен" скрыт маской, в соответствующую ячейку мы записываем единицу, остальные ячейки заполняем нулями;
 
 - вектор seg_input кодирует, какие "токены" относятся в первому предложению, а какие - ко второму: если "токен" принадлежит ко второму предложению, в соответствующую ячейку мы записываем единицу, остальные ячейки заполняем нулями.

**Кодирование позиции (Position Encoding)**

Поскольку нейросети архитектуры Transformer не являются рекуррентными, необходимо некоторым образом закодировать позицию каждого токена в каждом конкретном обучающем примере. Для этого используется техника, именуемая Position Encoding. Для каждого токена для каждого базиса в 512-мерном пространстве задается значение синусоиды или косинусоиды по следующим формулам:

![Position Encoding](/img/PositionEncoding2.png)

где pos - индекс позиции токена в тексте, а i - номер базиса в 512-мерном пространстве, т.е. i "пробегает" от 1 до 512.

Таким образом для каждого токена мы получаем 512-мерный вектор "времени". Далее мы прибавляем этот вектор к векторному представлению токена (также 512-мерному) и получаем тоже векторное представление токена, но с поправкой на его позицию в тексте.
