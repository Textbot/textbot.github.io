---
title: Выводы о нейросетях архитектуры Transformer
---

**Векторное представление словоформ (ВП)**

Transformer рассматривается как архитектура, позволяющая получать высокую точность ВП слов. В отличие, например, от ВП FastText (Т.Миколов),
сети BERT и GPT-2 обеспечивают ликвидацию морфологической и семантической омонимии. Это достигается за счет смещения дистрибутивной 
семантики словоформ механизмами внимания: на каждой итерации Attention-блока мы накладываем дистрибутивную модель синтактико-семантических связей в тексте на модель ВП. Однако, такой подход к моделированию ВП не всегда востребован: смещение дистрибутивной семантики может приводить к подмене одной словоформы другой, замене нескольких словоформ одной синонимичной или одной словоформы несколькими.

**Синтаксический анализ предложений**

В дистрибутивных моделях морфология и синтаксис не отделены от семантики, иными словами, форма и содержание текста едины. Поэтому, говорить о синтаксическом анализе текста в классическом понимании не приходится. Однако, анализ, визуализация и последующая дефаззификация матриц весов, соединяющих QueryLayer с KeyLayer внутри Attention-блока, позволяют нам делать вывод о способности нейросетей архитектуры Trandformer выделять синтактико-семантические связи между словоформами, в т.ч. объединять последовательности словоформ в токены, идентифицировать пары "слово-местоимение" и "токен-местоимение", определять различные типы синтактического и семантического подчинения и т.д.

**Семантический синтез текста**

...

**Моделирование семантики текста**

...
