---
title: Выводы о нейросетях архитектуры Transformer
---

**Векторное представление (ВП)**

Transformer рассматривается как архитектура, позволяющая получать высокую точность ВП слов. В отличие, например, от ВП FastText (Т.Миколов),
сети BERT и GPT-2 обеспечивают ликвидацию морфологической и семантической омонимии. Это достигается за счет смещения дистрибутивной 
семантики словоформ механизмами внимания: на каждой итерации Attention-блока мы накладываем модель синтактико-семантических связей в тексте 
на модель ВП. Однако, такой подход к моделированию ВП не всегда востребован: смещение дистрибутивной семантики может приводить к подмене 
одной словоформы другой, замене нескольких словоформ одной синонимичной или одной словоформы несколькими.

**Синтаксический анализ**
