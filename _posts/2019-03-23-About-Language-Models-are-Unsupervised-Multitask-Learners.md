Тезисно:
Смысл GPT-2: у вас есть текст из n слов/токенов. Скроем k слов/токенов, например, последнее слово. 
Создать такую сеть, чтобы на выходе мы получили исходный текст.

Обзор статьи "Language Models are Unsupervised Multitask Learners" за авторством создателей сети GPT-2.
Ссылка: https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf

I. Представление входных данных.
Используется подход, являющийся неким средним между представлением на уровне букв и представлением на уровне словоформ:
частотные словоформы подаются как есть, а нечастотные - как последовательности букв и/или буквенных n-грамм.

II. Что такое "attention head"?
В языковых моделях типа Transformer пространство векторных представлений разбивается на подпространства, в рамках которых кластеризуются некоторые метамодели синтаксических связей, именуемых "головами". Данный термин не случаен и заимствован из лингвистики. В частности, термин "Вершинное маркерование" (от англ. Head-marking) сообщает нам о способе моделирования синтаксической связи: от вершины (головы) к зависимым словоформам. Таким образом, задача обучения нейросети GPT-2 сводится к метамоделированию таких "голов".

![Attention Head](/img/AttentionHead.png)

Выше приведен пример того, выделяются "головы" в механизмах самовнимания. Читать изображение необходимо следующим образом: для каждого слова слева необходимо определить, насколько оно обращает внимание на каждое слово справа, находящее не "ниже" него. Слово справа, обладающее наибольшим весом, выступает "головой" конкретному слову слева. Так, например, слово run является зависимым элементов синтаксической связи, где "головой" выступает слово dog.

