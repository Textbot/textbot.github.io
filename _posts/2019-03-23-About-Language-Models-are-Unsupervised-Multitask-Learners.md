Тезисно:
Смысл GPT-2: у вас есть текст из n слов/токенов. Скроем k слов/токенов, например, последнее слово. 
Создать такую сеть, чтобы на выходе мы получили исходный текст.

Обзор статьи "Language Models are Unsupervised Multitask Learners" за авторством создателей сети GPT-2.
Ссылка: https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf

I. Представление входных данных.
Используется подход, являющийся неким средним между представлением на уровне букв и представлением на уровне словоформ:
частотные словоформы подаются как есть, а нечастотные - как последовательности букв и/или буквенных n-грамм.
