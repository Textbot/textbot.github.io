---
title: Частотное кодирование словоформ методом BPE
---

Хотя алгоритм BPE для кодирования и сжатия данных был предложен Ф. Гейджем в 1994-м году, применение BPE для дистрибутивных моделей в задачах NLP началось лишь в 2016-м с работы [Neural Machine Translation of Rare Words with Subword Units](https://www.aclweb.org/anthology/P16-1162) и была призвана решить проблему моделирования семантики редких словоформ, которых очень много, но встречаются они очень редко. Редкие словоформы было решено делить на токены. Такой подход позволяет смоделировать любую словоформу из нескольких токенов. Создание векторного представления для каждого токена позволяет моделировать дистрибутивную семантику словоформы, отсутствующей в словаре, по ВП токенов, из которых она состоит, а в паре с нейросетями архитектуры Transformer стало возможно изменять ВП еще и с учетом дистрибутивного контекста.

Технология BPE состоит из двух методов: моделирования словаря токенов и токенизации, т.е. разбиения словоформ на токены при уже сформированном словаре.  Размер словаря токенов может быть определен заранее, а может и не быть определен. Процесс моделирования словаря токенов следующий: вначале в словарь добавляются все символы, встречающиеся в тексте. Затем среди всех данных ищется пара символов, которая встречается наибольшее число раз, и заменяется но новый символ, точнее, такой паре присваивается уникальный номер и она добавляется в словарь. Такой процесс продолжается итеративно большое число раз. Если размер словаря токенов фиксирован, то работа алгоритма завершается, если все позиции в словаре заполнены. Если размер словаря токенов фиксирован, то работа завершается, когда все словоформы из обучающих данных оказываются в словаре.
Говоря о разбиении, поскольку зачастую существует несколько вариантов разбиения словоформ на токены, логичным развитие метода BPE стала модель ULM (Unigram Language Model), предложенная в <https://arxiv.org/pdf/1804.10959.pdf>, позволяющая оценить вероятности того или иного разбиения и выбрать наиболее вероятное. BPE и ULM положены в основу популярного токенизатора [Sentencepiece от Google](https://github.com/google/sentencepiece).

В 2017-м году немецкими исследователями Б. Хайнцерлингом и М. Струбе была опубликована работа ["BPEmb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages"](https://arxiv.org/pdf/1710.02187.pdf), а также выложены модели векторных представлений GloVe токенов, полученных методом BPE. Русскоязычные токены и ВП размером словаря от 1000 до 200000 токенов и размерностью пространства ВП от 25 до 300 доступны по ссылке <https://nlp.h-its.org/bpemb/ru/>.

...

**Выводы**

На сегодняшний день (апрель 2019-го) наилучшие результаты в большинстве задач NLP показывают нейросети архитектуры Transformer (BERT/GPT-2), на вход которых подаются не словоформы и не символы, а токены, полученные методом BPE. Логично предположить, что формирование токенов при обучение нейросети, а не вне ее, способно существенно повысить качество работы сетей.
