---
title: Частотное кодирование словоформ методом BPE
---

Хотя алгоритм BPE для кодирования и сжатия данных был предложен Ф. Гейджем в 1994-м году, применение BPE для дистрибутивных моделей в задачах NLP началось лишь в 2016-м с работы [Neural Machine Translation of Rare Words with Subword Units](https://www.aclweb.org/anthology/P16-1162) и была призвана решить проблемы редких словоформ, которых очень много, но встречаются они очень редко. Редкие словоформы было решено делить на токены. Такой подход позволяет смоделировать любую словоформу из нескольких токенов. Создание векторного представления для каждого токена позволяет моделировать дистрибутивную семантику словоформы, отсутствующей в словаре, по ВП токенов, из которых она состоит, а в паре с нейросетями архитектуры Transformer стало возможно изменять ВП еще и с учетом дистрибутивного контекста.
...

В 2017-м году немецкими исследователями Б. Хайнцерлингом и М. Струбе была опубликована работа ["BPEmb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages"](https://arxiv.org/pdf/1710.02187.pdf), а также выложены модели векторных представлений GloVe токенов, полученных методом BPE. Русскоязычные токены и ВП размером словаря от 1000 до 200000 токенов и размерностью пространства ВП от 25 до 300 доступны по ссылке <https://nlp.h-its.org/bpemb/ru/>.

...

**Выводы**

На сегодняшний день (апрель 2019-го) наилучшие результаты в большинстве задач NLP показывают нейросети архитектуры Transformer (BERT/GPT-2), на вход которых подаются не словоформы и не символы, а токены, полученные методом BPE. Логично предположить, что формирование токенов при обучение нейросети, а не вне ее, способно существенно повысить качество работы сетей.
