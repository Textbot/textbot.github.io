---
title: Частотное кодирование словоформ методом BPE
---

...

В 2017-м году немецкими исследователями Б. Хайнцерлингом и М. Струбе была опубликована работа ["BPEmb: Tokenization-free Pre-trained Subword Embeddings
in 275 Languages"](https://arxiv.org/pdf/1710.02187.pdf), а также выложены модели векторных представлений GloVe токенов, полученных методом BPE. Русскоязычные токены и ВП размером словаря от 1000 до 200000 токенов и размерностью пространства ВП от 25 до 300 доступны по ссылке
<https://nlp.h-its.org/bpemb/ru/>
