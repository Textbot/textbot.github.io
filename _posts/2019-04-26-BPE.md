---
title: Частотное кодирование словоформ методом BPE
---

...

В 2017-м году немецкими исследователями Б. Хайнцерлингом и М. Струбе была опубликована работа ["BPEmb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages"](https://arxiv.org/pdf/1710.02187.pdf), а также выложены модели векторных представлений GloVe токенов, полученных методом BPE. Русскоязычные токены и ВП размером словаря от 1000 до 200000 токенов и размерностью пространства ВП от 25 до 300 доступны по ссылке <https://nlp.h-its.org/bpemb/ru/>.

...

**Выводы**

На сегодняшний день (апрель 2019-го) наилучшие результаты в большинстве задач NLP показывают нейросети архитектуры Transformer (BERT/GPT-2), на вход которых подаются не словоформы и не символы, а токены, полученные методом BPE. Логично предположить, что формирование токенов при обучение нейросети, а не вне ее, способно существенно повысить качество работы сетей.
